{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "from torch import Tensor\n",
    "from scipy.special import gamma \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated()/ (1024*1024)\n",
    "    return 0\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class Fractional_Order_Matrix_Differential_Solver(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,input1,w,b,alpha,k,epoch):\n",
    "        alpha = torch.tensor(alpha)\n",
    "        k = torch.tensor(k)\n",
    "        epoch = torch.tensor(epoch)\n",
    "        ctx.save_for_backward(input1,w,b,alpha,k,epoch)\n",
    "        outputs = input1@w + b\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        input1,w,b,alpha,k,epoch = ctx.saved_tensors\n",
    "        x_fractional, w_fractional = Fractional_Order_Matrix_Differential_Solver.Fractional_Order_Matrix_Differential_Linear(input1,w,b,alpha,k,epoch)   \n",
    "        x_grad = torch.mm(grad_outputs,x_fractional)\n",
    "        w_grad = torch.mm(w_fractional,grad_outputs)\n",
    "        b_grad = grad_outputs.sum(dim=0)\n",
    "        return x_grad, w_grad, b_grad,None,None,None\n",
    "\n",
    "    @staticmethod\n",
    "    def Fractional_Order_Matrix_Differential_Linear(x,w,b,alpha,k,epoch):\n",
    "        #w\n",
    "        wf = w[:,0].view(1,-1)\n",
    "        #main\n",
    "        w_main = torch.mul(x,(torch.abs(wf)+1e-8)**(1-alpha)/gamma(2-alpha))\n",
    "        #partial\n",
    "        x_rows, x_cols = x.size()\n",
    "        bias = torch.full((x_rows, x_cols),b[0].item())\n",
    "        bias = bias.to(device)\n",
    "        w_partial = torch.mul(torch.mm(x,wf.T).view(-1,1).expand(-1,x_cols) - torch.mul(x,wf) + bias, torch.sign(wf)*(torch.abs(wf)+1e-8)**(-alpha)/gamma(1-alpha))\n",
    "        return w.T, (w_main + torch.exp(-k*epoch)*w_partial).T\n",
    "\n",
    "class FLinear(nn.Module):\n",
    "    \n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, alpha=0.9, k = 0.9, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "        self.weight = Parameter(torch.empty((in_features, out_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x, epoch):\n",
    "        return Fractional_Order_Matrix_Differential_Solver.apply(x, self.weight, self.bias, self.alpha, self.k, epoch)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}\"\n",
    "    \n",
    "def split(X,y):\n",
    "    X_train,X_temp,y_train,y_temp = train_test_split(X,y,test_size=0.3,shuffle=False)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333,shuffle=False)\n",
    "    return X_train,X_val,X_test,y_train,y_val,y_test\n",
    "\n",
    "#Mean Square Error\n",
    "def MSE(pred,true):\n",
    "    return np.mean((pred-true)**2)\n",
    "\n",
    "#Mean Absolute Error\n",
    "def MAE(pred, true):\n",
    "    return np.mean(np.abs(pred-true))\n",
    "\n",
    "def RMSE(pred,true):\n",
    "    return np.sqrt(np.mean((pred-true)**2))\n",
    "\n",
    "def MAPE(pred, true):\n",
    "    return np.mean(np.abs((pred - true) / true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_windows_size = 192  #i.e.,input length 192\n",
    "pred_length = 384     #i.e.,prediction lengths 384\n",
    "stock = 'DJI'    #ETTh2,DJI\n",
    "df_DJIA = pd.read_csv(r'./data/'+stock+'.csv')\n",
    "# del df_DJIA['date']        #ETT2\n",
    "del df_DJIA['Date']        #DJI\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "sca_DJIA = scaler.fit_transform(df_DJIA)\n",
    "\n",
    "features_j = 4     #ETTh2:6,DJI:4\n",
    "def create_sequences(data, slide_windows_size, pred_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - slide_windows_size - pred_length + 1):\n",
    "        X.append(data[i:i+slide_windows_size, :])  # sliding window size [seq_len, features]\n",
    "        y.append(data[i+slide_windows_size:i+slide_windows_size+pred_length, features_j])  \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(sca_DJIA, slide_windows_size, pred_length)\n",
    "X = torch.Tensor(X).to(device)\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train,X_val,X_test,y_train,y_val,y_test = split(X,y)   #7:2:1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0_0.01_0.1_0.1_0.1_RMSE:0.865125\n",
      "1.0_0.01_0.1_0.1_0.1_MAE:0.860487\n",
      "1.0_0.01_0.1_0.1_0.1_MAPE:93.083916\n",
      "1.0_0.01_0.1_0.1_0.01_RMSE:0.236520\n",
      "1.0_0.01_0.1_0.1_0.01_MAE:0.209617\n",
      "1.0_0.01_0.1_0.1_0.01_MAPE:0.336179\n",
      "1.0_0.01_0.1_0.1_0.01\n",
      "best_evaluation: 0.78231573\n",
      "1.0_0.01_0.1_0.1_0.001_RMSE:0.187794\n",
      "1.0_0.01_0.1_0.1_0.001_MAE:0.153760\n",
      "1.0_0.01_0.1_0.1_0.001_MAPE:0.230558\n",
      "1.0_0.01_0.1_0.1_0.001\n",
      "best_evaluation: 0.57211137\n",
      "1.0_0.01_0.1_0.1_0.0001_RMSE:0.212755\n",
      "1.0_0.01_0.1_0.1_0.0001_MAE:0.176063\n",
      "1.0_0.01_0.1_0.1_0.0001_MAPE:0.278526\n",
      "1.0_0.01_0.1_0.5_0.1_RMSE:0.865119\n",
      "1.0_0.01_0.1_0.5_0.1_MAE:0.860481\n",
      "1.0_0.01_0.1_0.5_0.1_MAPE:93.023186\n",
      "1.0_0.01_0.1_0.5_0.01_RMSE:0.235831\n",
      "1.0_0.01_0.1_0.5_0.01_MAE:0.208836\n",
      "1.0_0.01_0.1_0.5_0.01_MAPE:0.334580\n",
      "1.0_0.01_0.1_0.5_0.001_RMSE:0.186824\n",
      "1.0_0.01_0.1_0.5_0.001_MAE:0.151415\n",
      "1.0_0.01_0.1_0.5_0.001_MAPE:0.228141\n",
      "1.0_0.01_0.1_0.5_0.001\n",
      "best_evaluation: 0.5663805\n",
      "1.0_0.01_0.1_0.5_0.0001_RMSE:0.212734\n",
      "1.0_0.01_0.1_0.5_0.0001_MAE:0.176203\n",
      "1.0_0.01_0.1_0.5_0.0001_MAPE:0.278465\n",
      "1.0_0.01_0.1_0.9_0.1_RMSE:0.863586\n",
      "1.0_0.01_0.1_0.9_0.1_MAE:0.858899\n",
      "1.0_0.01_0.1_0.9_0.1_MAPE:2420.208740\n",
      "1.0_0.01_0.1_0.9_0.01_RMSE:0.232407\n",
      "1.0_0.01_0.1_0.9_0.01_MAE:0.204929\n",
      "1.0_0.01_0.1_0.9_0.01_MAPE:0.326653\n",
      "1.0_0.01_0.1_0.9_0.001_RMSE:0.186553\n",
      "1.0_0.01_0.1_0.9_0.001_MAE:0.151102\n",
      "1.0_0.01_0.1_0.9_0.001_MAPE:0.227812\n",
      "1.0_0.01_0.1_0.9_0.001\n",
      "best_evaluation: 0.5654673\n",
      "1.0_0.01_0.1_0.9_0.0001_RMSE:0.222900\n",
      "1.0_0.01_0.1_0.9_0.0001_MAE:0.186167\n",
      "1.0_0.01_0.1_0.9_0.0001_MAPE:0.300084\n",
      "1.0_0.01_0.05_0.1_0.1_RMSE:0.865128\n",
      "1.0_0.01_0.05_0.1_0.1_MAE:0.860489\n",
      "1.0_0.01_0.05_0.1_0.1_MAPE:93.113503\n",
      "1.0_0.01_0.05_0.1_0.01_RMSE:0.238085\n",
      "1.0_0.01_0.05_0.1_0.01_MAE:0.211384\n",
      "1.0_0.01_0.05_0.1_0.01_MAPE:0.339806\n",
      "1.0_0.01_0.05_0.1_0.001_RMSE:0.195258\n",
      "1.0_0.01_0.05_0.1_0.001_MAE:0.162930\n",
      "1.0_0.01_0.05_0.1_0.001_MAPE:0.244844\n",
      "1.0_0.01_0.05_0.1_0.0001_RMSE:0.210818\n",
      "1.0_0.01_0.05_0.1_0.0001_MAE:0.176440\n",
      "1.0_0.01_0.05_0.1_0.0001_MAPE:0.273451\n",
      "1.0_0.01_0.05_0.5_0.1_RMSE:0.865124\n",
      "1.0_0.01_0.05_0.5_0.1_MAE:0.860486\n",
      "1.0_0.01_0.05_0.5_0.1_MAPE:93.077866\n",
      "1.0_0.01_0.05_0.5_0.01_RMSE:0.236871\n",
      "1.0_0.01_0.05_0.5_0.01_MAE:0.210012\n",
      "1.0_0.01_0.05_0.5_0.01_MAPE:0.336991\n",
      "1.0_0.01_0.05_0.5_0.001_RMSE:0.188490\n",
      "1.0_0.01_0.05_0.5_0.001_MAE:0.154719\n",
      "1.0_0.01_0.05_0.5_0.001_MAPE:0.231967\n",
      "1.0_0.01_0.05_0.5_0.0001_RMSE:0.211722\n",
      "1.0_0.01_0.05_0.5_0.0001_MAE:0.175823\n",
      "1.0_0.01_0.05_0.5_0.0001_MAPE:0.276243\n",
      "1.0_0.01_0.05_0.9_0.1_RMSE:0.864685\n",
      "1.0_0.01_0.05_0.9_0.1_MAE:0.860039\n",
      "1.0_0.01_0.05_0.9_0.1_MAPE:93.924690\n",
      "1.0_0.01_0.05_0.9_0.01_RMSE:0.233872\n",
      "1.0_0.01_0.05_0.9_0.01_MAE:0.206610\n",
      "1.0_0.01_0.05_0.9_0.01_MAPE:0.330058\n",
      "1.0_0.01_0.05_0.9_0.001_RMSE:0.187274\n",
      "1.0_0.01_0.05_0.9_0.001_MAE:0.151608\n",
      "1.0_0.01_0.05_0.9_0.001_MAPE:0.229014\n",
      "1.0_0.01_0.05_0.9_0.0001_RMSE:0.217458\n",
      "1.0_0.01_0.05_0.9_0.0001_MAE:0.180843\n",
      "1.0_0.01_0.05_0.9_0.0001_MAPE:0.288414\n",
      "1.0_0.01_0.03_0.1_0.1_RMSE:0.865129\n",
      "1.0_0.01_0.03_0.1_0.1_MAE:0.860491\n",
      "1.0_0.01_0.03_0.1_0.1_MAPE:93.126602\n",
      "1.0_0.01_0.03_0.1_0.01_RMSE:0.243025\n",
      "1.0_0.01_0.03_0.1_0.01_MAE:0.216899\n",
      "1.0_0.01_0.03_0.1_0.01_MAPE:0.351337\n",
      "1.0_0.01_0.03_0.1_0.001_RMSE:0.205764\n",
      "1.0_0.01_0.03_0.1_0.001_MAE:0.173386\n",
      "1.0_0.01_0.03_0.1_0.001_MAPE:0.264186\n",
      "1.0_0.01_0.03_0.1_0.0001_RMSE:0.214114\n",
      "1.0_0.01_0.03_0.1_0.0001_MAE:0.180275\n",
      "1.0_0.01_0.03_0.1_0.0001_MAPE:0.278251\n",
      "1.0_0.01_0.03_0.5_0.1_RMSE:0.865127\n",
      "1.0_0.01_0.03_0.5_0.1_MAE:0.860488\n",
      "1.0_0.01_0.03_0.5_0.1_MAPE:93.102707\n",
      "1.0_0.01_0.03_0.5_0.01_RMSE:0.237722\n",
      "1.0_0.01_0.03_0.5_0.01_MAE:0.210974\n",
      "1.0_0.01_0.03_0.5_0.01_MAPE:0.338964\n",
      "1.0_0.01_0.03_0.5_0.001_RMSE:0.194070\n",
      "1.0_0.01_0.03_0.5_0.001_MAE:0.161660\n",
      "1.0_0.01_0.03_0.5_0.001_MAPE:0.242649\n",
      "1.0_0.01_0.03_0.5_0.0001_RMSE:0.210720\n",
      "1.0_0.01_0.03_0.5_0.0001_MAE:0.176087\n",
      "1.0_0.01_0.03_0.5_0.0001_MAPE:0.273505\n",
      "1.0_0.01_0.03_0.9_0.1_RMSE:0.865106\n",
      "1.0_0.01_0.03_0.9_0.1_MAE:0.860468\n",
      "1.0_0.01_0.03_0.9_0.1_MAPE:92.891174\n",
      "1.0_0.01_0.03_0.9_0.01_RMSE:0.234947\n",
      "1.0_0.01_0.03_0.9_0.01_MAE:0.207838\n",
      "1.0_0.01_0.03_0.9_0.01_MAPE:0.332538\n",
      "1.0_0.01_0.03_0.9_0.001_RMSE:0.187348\n",
      "1.0_0.01_0.03_0.9_0.001_MAE:0.151633\n",
      "1.0_0.01_0.03_0.9_0.001_MAPE:0.229057\n",
      "1.0_0.01_0.03_0.9_0.0001_RMSE:0.216780\n",
      "1.0_0.01_0.03_0.9_0.0001_MAE:0.179720\n",
      "1.0_0.01_0.03_0.9_0.0001_MAPE:0.287084\n"
     ]
    }
   ],
   "source": [
    "alpha = 1.0   ####0.7,0.8,0.9,1.0\n",
    "k = 0.01   #In integer order, k does not play a role.\n",
    "\n",
    "lrs =[0.1,0.05,0.03]                     #4e-2 \n",
    "momentums =[0.1,0.5,0.9]                                        #0.9 \n",
    "weight_decays =[0.1,0.01,0.001,0.0001]                                            #1e-2\n",
    "\n",
    "num_feature = 5     #ETTh1:7,DJI:5\n",
    "batch_size = 256\n",
    "set_seed()\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1=256, hidden_size2=128,output_size=pred_length):   ###DJI:hidden_size=256,ETTh1:hidden_size=128.\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # self.linear1 = FLinear(input_size, hidden_size1, alpha, k)  \n",
    "        self.linear1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.leakrelu1 = nn.LeakyReLU()                          \n",
    "        # self.linear2 = FLinear(hidden_size1, hidden_size2, alpha, k) \n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.leakrelu2 = nn.LeakyReLU()\n",
    "        # self.linear3 = FLinear(hidden_size2, output_size, alpha, k)   \n",
    "        self.linear3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x, epoch=0):\n",
    "        x = self.flatten(x)    # (batch_size, seq_len*num_features)\n",
    "        # x = self.leakrelu1(self.linear1(x, epoch)) \n",
    "        x = self.leakrelu1(self.linear1(x))\n",
    "        # x = self.leakrelu2(self.linear2(x, epoch))\n",
    "        x = self.leakrelu2(self.linear2(x))\n",
    "        # x = self.linear3(x, epoch)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "lr_best = 0\n",
    "momentum_best = 0\n",
    "weight_decay_best = 0\n",
    "best_evaluation = 10\n",
    "\n",
    "for lr in lrs:\n",
    "    for momentum in momentums:\n",
    "        for weight_decay in weight_decays:\n",
    "            set_seed()\n",
    "            model = MLP(input_size=slide_windows_size*num_feature).to(device)\n",
    "            num_epochs = 500   #\n",
    "            best_loss = 10\n",
    "            criterion = nn.MSELoss()\n",
    "            # optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-2)\n",
    "            optimizer = torch.optim.SGD(model.parameters(),lr=lr,momentum=momentum,weight_decay=weight_decay)\n",
    "            for ii in range(num_epochs):\n",
    "                model.train()\n",
    "                loss_sum = 0\n",
    "                for inputs, targets in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs,ii)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss_sum += loss\n",
    "                    loss.backward()   #The default value of retain_graph is False.\n",
    "                    optimizer.step()\n",
    "                # train_loss10.append(loss_sum.cpu().detach().numpy())     ###########\n",
    "                \n",
    "                # print(f\"Epoch {ii + 1}/{num_epochs}, Train Loss: {loss_sum.cpu().detach().numpy():.4f}\")\n",
    "                    \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    Val_outputs = model(X_val)\n",
    "                    MSE_val = MSE(y_val.cpu().detach().numpy(),Val_outputs.cpu().detach().numpy())\n",
    "                    \n",
    "                    # val_loss10.append(MSE_val)   ########################Validation_loss\n",
    "                    \n",
    "                    # print(f\"Epoch {ii + 1}/{num_epochs}, Val Loss: {MSE_val:.4f}\")\n",
    "                    # print('')\n",
    "                    if best_loss > MSE_val:\n",
    "                        best_loss = MSE_val\n",
    "                        torch.save(model.state_dict(), r'./model/table2/'+stock+'_model_fractional_'+str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_.pth') \n",
    "\n",
    "            model.load_state_dict(torch.load('./model/table2/'+stock+'_model_fractional_'+str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_.pth'))\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs = model(X_test)\n",
    "            RMSE10 = RMSE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "            MAE10 = MAE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "            MAPE10 = MAPE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "            print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_'+f'RMSE:{RMSE10:.6f}')\n",
    "            print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_'+f'MAE:{MAE10:.6f}')\n",
    "            print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_'+f'MAPE:{MAPE10:.6f}')\n",
    "            if best_evaluation > RMSE10 + MAE10 + MAPE10:\n",
    "                best_evaluation = RMSE10 + MAE10 + MAPE10\n",
    "                print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay))\n",
    "                print('best_evaluation:',best_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5654673"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8_0.005_0.1_0.9_0.001_RMSE:0.185478\n",
      "0.8_0.005_0.1_0.9_0.001_MAE:0.154724\n",
      "0.8_0.005_0.1_0.9_0.001_MAPE:0.210638\n",
      "0.8_0.005_0.1_0.9_0.001\n",
      "best_evaluation: 0.55084056\n",
      "0.8_0.01_0.1_0.9_0.001_RMSE:0.170215\n",
      "0.8_0.01_0.1_0.9_0.001_MAE:0.143907\n",
      "0.8_0.01_0.1_0.9_0.001_MAPE:0.197538\n",
      "0.8_0.01_0.1_0.9_0.001\n",
      "best_evaluation: 0.51165974\n",
      "0.8_0.05_0.1_0.9_0.001_RMSE:0.204546\n",
      "0.8_0.05_0.1_0.9_0.001_MAE:0.170802\n",
      "0.8_0.05_0.1_0.9_0.001_MAPE:0.262138\n",
      "0.8_0.1_0.1_0.9_0.001_RMSE:0.187661\n",
      "0.8_0.1_0.1_0.9_0.001_MAE:0.156263\n",
      "0.8_0.1_0.1_0.9_0.001_MAPE:0.212264\n",
      "0.8_0.5_0.1_0.9_0.001_RMSE:0.192001\n",
      "0.8_0.5_0.1_0.9_0.001_MAE:0.161765\n",
      "0.8_0.5_0.1_0.9_0.001_MAPE:0.235442\n",
      "0.85_0.005_0.1_0.9_0.001_RMSE:0.188668\n",
      "0.85_0.005_0.1_0.9_0.001_MAE:0.157837\n",
      "0.85_0.005_0.1_0.9_0.001_MAPE:0.220020\n",
      "0.85_0.01_0.1_0.9_0.001_RMSE:0.187836\n",
      "0.85_0.01_0.1_0.9_0.001_MAE:0.157617\n",
      "0.85_0.01_0.1_0.9_0.001_MAPE:0.218301\n",
      "0.85_0.05_0.1_0.9_0.001_RMSE:0.198446\n",
      "0.85_0.05_0.1_0.9_0.001_MAE:0.161253\n",
      "0.85_0.05_0.1_0.9_0.001_MAPE:0.236569\n",
      "0.85_0.1_0.1_0.9_0.001_RMSE:0.207879\n",
      "0.85_0.1_0.1_0.9_0.001_MAE:0.173449\n",
      "0.85_0.1_0.1_0.9_0.001_MAPE:0.268115\n",
      "0.85_0.5_0.1_0.9_0.001_RMSE:0.184683\n",
      "0.85_0.5_0.1_0.9_0.001_MAE:0.154589\n",
      "0.85_0.5_0.1_0.9_0.001_MAPE:0.222033\n",
      "0.9_0.005_0.1_0.9_0.001_RMSE:0.191117\n",
      "0.9_0.005_0.1_0.9_0.001_MAE:0.160028\n",
      "0.9_0.005_0.1_0.9_0.001_MAPE:0.235494\n",
      "0.9_0.01_0.1_0.9_0.001_RMSE:0.203716\n",
      "0.9_0.01_0.1_0.9_0.001_MAE:0.169528\n",
      "0.9_0.01_0.1_0.9_0.001_MAPE:0.260538\n",
      "0.9_0.05_0.1_0.9_0.001_RMSE:0.211291\n",
      "0.9_0.05_0.1_0.9_0.001_MAE:0.171033\n",
      "0.9_0.05_0.1_0.9_0.001_MAPE:0.194119\n",
      "0.9_0.1_0.1_0.9_0.001_RMSE:0.187482\n",
      "0.9_0.1_0.1_0.9_0.001_MAE:0.157986\n",
      "0.9_0.1_0.1_0.9_0.001_MAPE:0.225602\n",
      "0.9_0.5_0.1_0.9_0.001_RMSE:0.190318\n",
      "0.9_0.5_0.1_0.9_0.001_MAE:0.157439\n",
      "0.9_0.5_0.1_0.9_0.001_MAPE:0.235813\n",
      "0.95_0.005_0.1_0.9_0.001_RMSE:0.179688\n",
      "0.95_0.005_0.1_0.9_0.001_MAE:0.147760\n",
      "0.95_0.005_0.1_0.9_0.001_MAPE:0.216776\n",
      "0.95_0.01_0.1_0.9_0.001_RMSE:0.155400\n",
      "0.95_0.01_0.1_0.9_0.001_MAE:0.134286\n",
      "0.95_0.01_0.1_0.9_0.001_MAPE:0.162255\n",
      "0.95_0.01_0.1_0.9_0.001\n",
      "best_evaluation: 0.45194203\n",
      "0.95_0.05_0.1_0.9_0.001_RMSE:0.185695\n",
      "0.95_0.05_0.1_0.9_0.001_MAE:0.156025\n",
      "0.95_0.05_0.1_0.9_0.001_MAPE:0.224782\n",
      "0.95_0.1_0.1_0.9_0.001_RMSE:0.186791\n",
      "0.95_0.1_0.1_0.9_0.001_MAE:0.154286\n",
      "0.95_0.1_0.1_0.9_0.001_MAPE:0.228864\n",
      "0.95_0.5_0.1_0.9_0.001_RMSE:0.189481\n",
      "0.95_0.5_0.1_0.9_0.001_MAE:0.159253\n",
      "0.95_0.5_0.1_0.9_0.001_MAPE:0.232870\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.8,0.85,0.9,0.95]   ####0.7,0.8,0.9,1.0\n",
    "ks = [0.005,0.01,0.05,0.1,0.5]   \n",
    "\n",
    "lr = 0.1               \n",
    "momentums =0.9                                       \n",
    "weight_decay =1e-3                                        \n",
    "\n",
    "num_feature = 5     #ETTh1:7,DJI:5\n",
    "batch_size = 256\n",
    "set_seed()\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1=256, hidden_size2=128,output_size=pred_length):   ###DJI:hidden_size=256,ETTh1:hidden_size=128.\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = FLinear(input_size, hidden_size1, alpha, k)  \n",
    "        self.leakrelu1 = nn.LeakyReLU()                          \n",
    "        self.linear2 = FLinear(hidden_size1, hidden_size2, alpha, k) \n",
    "        self.leakrelu2 = nn.LeakyReLU()\n",
    "        self.linear3 = FLinear(hidden_size2, output_size, alpha, k)   \n",
    "\n",
    "    def forward(self, x, epoch=0):\n",
    "        x = self.flatten(x)    # (batch_size, seq_len*num_features)\n",
    "        x = self.leakrelu1(self.linear1(x, epoch)) \n",
    "        x = self.leakrelu2(self.linear2(x, epoch))\n",
    "        x = self.linear3(x, epoch)\n",
    "        return x\n",
    "\n",
    "for alpha in alphas:\n",
    "    for k in ks:\n",
    "        set_seed()\n",
    "        model = MLP(input_size=slide_windows_size*num_feature).to(device)\n",
    "        num_epochs = 500   #\n",
    "        best_loss = 10\n",
    "        criterion = nn.MSELoss()\n",
    "        # optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-2)\n",
    "        optimizer = torch.optim.SGD(model.parameters(),lr=lr,momentum=momentum,weight_decay=weight_decay)\n",
    "        for ii in range(num_epochs):\n",
    "            model.train()\n",
    "            loss_sum = 0\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs,ii)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss_sum += loss\n",
    "                loss.backward()   #The default value of retain_graph is False.\n",
    "                optimizer.step()\n",
    "            # train_loss10.append(loss_sum.cpu().detach().numpy())     ###########\n",
    "            \n",
    "            # print(f\"Epoch {ii + 1}/{num_epochs}, Train Loss: {loss_sum.cpu().detach().numpy():.4f}\")\n",
    "                \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Val_outputs = model(X_val)\n",
    "                MSE_val = MSE(y_val.cpu().detach().numpy(),Val_outputs.cpu().detach().numpy())\n",
    "                \n",
    "                # val_loss10.append(MSE_val)   ########################Validation_loss\n",
    "                \n",
    "                # print(f\"Epoch {ii + 1}/{num_epochs}, Val Loss: {MSE_val:.4f}\")\n",
    "                # print('')\n",
    "                if best_loss > MSE_val:\n",
    "                    best_loss = MSE_val\n",
    "                    torch.save(model.state_dict(), r'./model/table2/'+stock+'_model_fractional_f_'+str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_.pth') \n",
    "\n",
    "        model.load_state_dict(torch.load('./model/table2/'+stock+'_model_fractional_f_'+str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_.pth'))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "        RMSE10 = RMSE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        MAE10 = MAE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        MAPE10 = MAPE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_'+f'RMSE:{RMSE10:.6f}')\n",
    "        print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_'+f'MAE:{MAE10:.6f}')\n",
    "        print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_'+f'MAPE:{MAPE10:.6f}')\n",
    "        if best_evaluation > RMSE10 + MAE10 + MAPE10:\n",
    "            best_evaluation = RMSE10 + MAE10 + MAPE10\n",
    "            print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay))\n",
    "            print('best_evaluation:',best_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_windows_size = 192  #i.e.,input length 192\n",
    "pred_length = 384     #i.e.,prediction lengths 384\n",
    "stock = 'ETTh2'    #ETTh2,DJI\n",
    "df_DJIA = pd.read_csv(r'./data/'+stock+'.csv')\n",
    "del df_DJIA['date']        #ETT2\n",
    "# del df_DJIA['Date']        #DJI\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "sca_DJIA = scaler.fit_transform(df_DJIA)\n",
    "\n",
    "features_j = 6     #ETTh2:6,DJI:4\n",
    "def create_sequences(data, slide_windows_size, pred_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - slide_windows_size - pred_length + 1):\n",
    "        X.append(data[i:i+slide_windows_size, :])  # sliding window size [seq_len, features]\n",
    "        y.append(data[i+slide_windows_size:i+slide_windows_size+pred_length, features_j])  \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(sca_DJIA, slide_windows_size, pred_length)\n",
    "X = torch.Tensor(X).to(device)\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train,X_val,X_test,y_train,y_val,y_test = split(X,y)   #7:2:1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0_0.01_0.01_0.1_0.01_RMSE:0.111223\n",
      "1.0_0.01_0.01_0.1_0.01_MAE:0.087949\n",
      "1.0_0.01_0.01_0.1_0.01_MAPE:0.167158\n",
      "1.0_0.01_0.01_0.1_0.01\n",
      "best_evaluation: 0.36633083\n",
      "1.0_0.01_0.01_0.1_0.001_RMSE:0.107440\n",
      "1.0_0.01_0.01_0.1_0.001_MAE:0.086532\n",
      "1.0_0.01_0.01_0.1_0.001_MAPE:0.156864\n",
      "1.0_0.01_0.01_0.1_0.001\n",
      "best_evaluation: 0.3508362\n",
      "1.0_0.01_0.01_0.1_0.0001_RMSE:0.107330\n",
      "1.0_0.01_0.01_0.1_0.0001_MAE:0.086617\n",
      "1.0_0.01_0.01_0.1_0.0001_MAPE:0.156034\n",
      "1.0_0.01_0.01_0.1_0.0001\n",
      "best_evaluation: 0.3499819\n",
      "1.0_0.01_0.01_0.5_0.01_RMSE:0.111372\n",
      "1.0_0.01_0.01_0.5_0.01_MAE:0.088036\n",
      "1.0_0.01_0.01_0.5_0.01_MAPE:0.167517\n",
      "1.0_0.01_0.01_0.5_0.001_RMSE:0.107191\n",
      "1.0_0.01_0.01_0.5_0.001_MAE:0.086377\n",
      "1.0_0.01_0.01_0.5_0.001_MAPE:0.156200\n",
      "1.0_0.01_0.01_0.5_0.001\n",
      "best_evaluation: 0.34976786\n",
      "1.0_0.01_0.01_0.5_0.0001_RMSE:0.106825\n",
      "1.0_0.01_0.01_0.5_0.0001_MAE:0.086327\n",
      "1.0_0.01_0.01_0.5_0.0001_MAPE:0.154558\n",
      "1.0_0.01_0.01_0.5_0.0001\n",
      "best_evaluation: 0.34770986\n",
      "1.0_0.01_0.01_0.9_0.01_RMSE:0.112342\n",
      "1.0_0.01_0.01_0.9_0.01_MAE:0.088620\n",
      "1.0_0.01_0.01_0.9_0.01_MAPE:0.169718\n",
      "1.0_0.01_0.01_0.9_0.001_RMSE:0.107135\n",
      "1.0_0.01_0.01_0.9_0.001_MAE:0.086223\n",
      "1.0_0.01_0.01_0.9_0.001_MAPE:0.156269\n",
      "1.0_0.01_0.01_0.9_0.0001_RMSE:0.104099\n",
      "1.0_0.01_0.01_0.9_0.0001_MAE:0.084187\n",
      "1.0_0.01_0.01_0.9_0.0001_MAPE:0.148542\n",
      "1.0_0.01_0.01_0.9_0.0001\n",
      "best_evaluation: 0.336828\n",
      "1.0_0.01_0.005_0.1_0.01_RMSE:0.110783\n",
      "1.0_0.01_0.005_0.1_0.01_MAE:0.087688\n",
      "1.0_0.01_0.005_0.1_0.01_MAPE:0.166127\n",
      "1.0_0.01_0.005_0.1_0.001_RMSE:0.107701\n",
      "1.0_0.01_0.005_0.1_0.001_MAE:0.086765\n",
      "1.0_0.01_0.005_0.1_0.001_MAPE:0.157342\n",
      "1.0_0.01_0.005_0.1_0.0001_RMSE:0.107737\n",
      "1.0_0.01_0.005_0.1_0.0001_MAE:0.086927\n",
      "1.0_0.01_0.005_0.1_0.0001_MAPE:0.156969\n",
      "1.0_0.01_0.005_0.5_0.01_RMSE:0.111123\n",
      "1.0_0.01_0.005_0.5_0.01_MAE:0.087890\n",
      "1.0_0.01_0.005_0.5_0.01_MAPE:0.166926\n",
      "1.0_0.01_0.005_0.5_0.001_RMSE:0.107506\n",
      "1.0_0.01_0.005_0.5_0.001_MAE:0.086573\n",
      "1.0_0.01_0.005_0.5_0.001_MAPE:0.157020\n",
      "1.0_0.01_0.005_0.5_0.0001_RMSE:0.107432\n",
      "1.0_0.01_0.005_0.5_0.0001_MAE:0.086676\n",
      "1.0_0.01_0.005_0.5_0.0001_MAPE:0.156299\n",
      "1.0_0.01_0.005_0.9_0.01_RMSE:0.111773\n",
      "1.0_0.01_0.005_0.9_0.01_MAE:0.088279\n",
      "1.0_0.01_0.005_0.9_0.01_MAPE:0.168425\n",
      "1.0_0.01_0.005_0.9_0.001_RMSE:0.107026\n",
      "1.0_0.01_0.005_0.9_0.001_MAE:0.086271\n",
      "1.0_0.01_0.005_0.9_0.001_MAPE:0.155726\n",
      "1.0_0.01_0.005_0.9_0.0001_RMSE:0.105189\n",
      "1.0_0.01_0.005_0.9_0.0001_MAE:0.085190\n",
      "1.0_0.01_0.005_0.9_0.0001_MAPE:0.150392\n",
      "1.0_0.01_0.001_0.1_0.01_RMSE:0.124442\n",
      "1.0_0.01_0.001_0.1_0.01_MAE:0.097674\n",
      "1.0_0.01_0.001_0.1_0.01_MAPE:0.196800\n",
      "1.0_0.01_0.001_0.1_0.001_RMSE:0.115076\n",
      "1.0_0.01_0.001_0.1_0.001_MAE:0.090536\n",
      "1.0_0.01_0.001_0.1_0.001_MAPE:0.175348\n",
      "1.0_0.01_0.001_0.1_0.0001_RMSE:0.114289\n",
      "1.0_0.01_0.001_0.1_0.0001_MAE:0.090019\n",
      "1.0_0.01_0.001_0.1_0.0001_MAPE:0.173578\n",
      "1.0_0.01_0.001_0.5_0.01_RMSE:0.112996\n",
      "1.0_0.01_0.001_0.5_0.01_MAE:0.089041\n",
      "1.0_0.01_0.001_0.5_0.01_MAPE:0.170972\n",
      "1.0_0.01_0.001_0.5_0.001_RMSE:0.107867\n",
      "1.0_0.01_0.001_0.5_0.001_MAE:0.086779\n",
      "1.0_0.01_0.001_0.5_0.001_MAPE:0.158001\n",
      "1.0_0.01_0.001_0.5_0.0001_RMSE:0.107766\n",
      "1.0_0.01_0.001_0.5_0.0001_MAE:0.086914\n",
      "1.0_0.01_0.001_0.5_0.0001_MAPE:0.157279\n",
      "1.0_0.01_0.001_0.9_0.01_RMSE:0.111045\n",
      "1.0_0.01_0.001_0.9_0.01_MAE:0.087844\n",
      "1.0_0.01_0.001_0.9_0.01_MAPE:0.166764\n",
      "1.0_0.01_0.001_0.9_0.001_RMSE:0.107505\n",
      "1.0_0.01_0.001_0.9_0.001_MAE:0.086585\n",
      "1.0_0.01_0.001_0.9_0.001_MAPE:0.156978\n",
      "1.0_0.01_0.001_0.9_0.0001_RMSE:0.107427\n",
      "1.0_0.01_0.001_0.9_0.0001_MAE:0.086690\n",
      "1.0_0.01_0.001_0.9_0.0001_MAPE:0.156228\n",
      "1.0_0.01_0.0005_0.1_0.01_RMSE:0.144726\n",
      "1.0_0.01_0.0005_0.1_0.01_MAE:0.116110\n",
      "1.0_0.01_0.0005_0.1_0.01_MAPE:0.248746\n",
      "1.0_0.01_0.0005_0.1_0.001_RMSE:0.138904\n",
      "1.0_0.01_0.0005_0.1_0.001_MAE:0.110554\n",
      "1.0_0.01_0.0005_0.1_0.001_MAPE:0.232845\n",
      "1.0_0.01_0.0005_0.1_0.0001_RMSE:0.138336\n",
      "1.0_0.01_0.0005_0.1_0.0001_MAE:0.110023\n",
      "1.0_0.01_0.0005_0.1_0.0001_MAPE:0.231333\n",
      "1.0_0.01_0.0005_0.5_0.01_RMSE:0.127400\n",
      "1.0_0.01_0.0005_0.5_0.01_MAE:0.100176\n",
      "1.0_0.01_0.0005_0.5_0.01_MAPE:0.203838\n",
      "1.0_0.01_0.0005_0.5_0.001_RMSE:0.118122\n",
      "1.0_0.01_0.0005_0.5_0.001_MAE:0.092722\n",
      "1.0_0.01_0.0005_0.5_0.001_MAPE:0.182136\n",
      "1.0_0.01_0.0005_0.5_0.0001_RMSE:0.117283\n",
      "1.0_0.01_0.0005_0.5_0.0001_MAE:0.092112\n",
      "1.0_0.01_0.0005_0.5_0.0001_MAPE:0.180233\n",
      "1.0_0.01_0.0005_0.9_0.01_RMSE:0.110694\n",
      "1.0_0.01_0.0005_0.9_0.01_MAE:0.087636\n",
      "1.0_0.01_0.0005_0.9_0.01_MAPE:0.165913\n",
      "1.0_0.01_0.0005_0.9_0.001_RMSE:0.107704\n",
      "1.0_0.01_0.0005_0.9_0.001_MAE:0.086818\n",
      "1.0_0.01_0.0005_0.9_0.001_MAPE:0.157218\n",
      "1.0_0.01_0.0005_0.9_0.0001_RMSE:0.107758\n",
      "1.0_0.01_0.0005_0.9_0.0001_MAE:0.086992\n",
      "1.0_0.01_0.0005_0.9_0.0001_MAPE:0.156885\n"
     ]
    }
   ],
   "source": [
    "alpha = 1.0   ####0.7,0.8,0.9,1.0\n",
    "k = 0.01    #In integer order, k does not play a role.\n",
    "\n",
    "lrs =[0.01,0.005,0.001,0.0005]                     #ETTh2\n",
    "momentums =[0.1,0.5,0.9]                                         #0.9 \n",
    "weight_decays =[0.01,0.001,0.0001]                                            #1e-2\n",
    "\n",
    "num_feature = 7     #ETTh1:7,DJI:5\n",
    "batch_size = 256\n",
    "set_seed()\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1=256, hidden_size2=128,output_size=pred_length):   ###DJI:hidden_size=256,ETTh1:hidden_size=128.\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # self.linear1 = FLinear(input_size, hidden_size1, alpha, k)  \n",
    "        self.linear1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.leakrelu1 = nn.LeakyReLU()                          \n",
    "        # self.linear2 = FLinear(hidden_size1, hidden_size2, alpha, k) \n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.leakrelu2 = nn.LeakyReLU()\n",
    "        # self.linear3 = FLinear(hidden_size2, output_size, alpha, k)   \n",
    "        self.linear3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x, epoch=0):\n",
    "        x = self.flatten(x)    # (batch_size, seq_len*num_features)\n",
    "        # x = self.leakrelu1(self.linear1(x, epoch)) \n",
    "        x = self.leakrelu1(self.linear1(x))\n",
    "        # x = self.leakrelu2(self.linear2(x, epoch))\n",
    "        x = self.leakrelu2(self.linear2(x))\n",
    "        # x = self.linear3(x, epoch)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "lr_best = 0\n",
    "momentum_best = 0\n",
    "weight_decay_best = 0\n",
    "best_evaluation = 10\n",
    "\n",
    "for lr in lrs:\n",
    "    for momentum in momentums:\n",
    "        for weight_decay in weight_decays:\n",
    "            set_seed()\n",
    "            model = MLP(input_size=slide_windows_size*num_feature).to(device)\n",
    "            num_epochs = 500   #\n",
    "            best_loss = 10\n",
    "            criterion = nn.MSELoss()\n",
    "            # optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-2)\n",
    "            optimizer = torch.optim.SGD(model.parameters(),lr=lr,momentum=momentum,weight_decay=weight_decay)\n",
    "            for ii in range(num_epochs):\n",
    "                model.train()\n",
    "                loss_sum = 0\n",
    "                for inputs, targets in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs,ii)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss_sum += loss\n",
    "                    loss.backward()   #The default value of retain_graph is False.\n",
    "                    optimizer.step()\n",
    "                # train_loss10.append(loss_sum.cpu().detach().numpy())     ###########\n",
    "                \n",
    "                # print(f\"Epoch {ii + 1}/{num_epochs}, Train Loss: {loss_sum.cpu().detach().numpy():.4f}\")\n",
    "                    \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    Val_outputs = model(X_val)\n",
    "                    MSE_val = MSE(y_val.cpu().detach().numpy(),Val_outputs.cpu().detach().numpy())\n",
    "                    \n",
    "                    # val_loss10.append(MSE_val)   ########################Validation_loss\n",
    "                    \n",
    "                    # print(f\"Epoch {ii + 1}/{num_epochs}, Val Loss: {MSE_val:.4f}\")\n",
    "                    # print('')\n",
    "                    if best_loss > MSE_val:\n",
    "                        best_loss = MSE_val\n",
    "                        torch.save(model.state_dict(), r'./model/table2/'+stock+'_model_fractional_'+str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_.pth') \n",
    "\n",
    "            model.load_state_dict(torch.load('./model/table2/'+stock+'_model_fractional_'+str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_.pth'))\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs = model(X_test)\n",
    "            RMSE10 = RMSE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "            MAE10 = MAE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "            MAPE10 = MAPE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "            print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_'+f'RMSE:{RMSE10:.6f}')\n",
    "            print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_'+f'MAE:{MAE10:.6f}')\n",
    "            print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_'+f'MAPE:{MAPE10:.6f}')\n",
    "            if best_evaluation > RMSE10 + MAE10 + MAPE10:\n",
    "                best_evaluation = RMSE10 + MAE10 + MAPE10\n",
    "                print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay))\n",
    "                print('best_evaluation:',best_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8_0.005_0.01_0.9_0.0001_RMSE:0.136359\n",
      "0.8_0.005_0.01_0.9_0.0001_MAE:0.110283\n",
      "0.8_0.005_0.01_0.9_0.0001_MAPE:0.181609\n",
      "0.8_0.01_0.01_0.9_0.0001_RMSE:0.133766\n",
      "0.8_0.01_0.01_0.9_0.0001_MAE:0.108530\n",
      "0.8_0.01_0.01_0.9_0.0001_MAPE:0.190440\n",
      "0.8_0.05_0.01_0.9_0.0001_RMSE:0.104112\n",
      "0.8_0.05_0.01_0.9_0.0001_MAE:0.084587\n",
      "0.8_0.05_0.01_0.9_0.0001_MAPE:0.146903\n",
      "0.8_0.05_0.01_0.9_0.0001\n",
      "best_evaluation: 0.3356028\n",
      "0.8_0.1_0.01_0.9_0.0001_RMSE:0.108161\n",
      "0.8_0.1_0.01_0.9_0.0001_MAE:0.088050\n",
      "0.8_0.1_0.01_0.9_0.0001_MAPE:0.154006\n",
      "0.8_0.5_0.01_0.9_0.0001_RMSE:0.106071\n",
      "0.8_0.5_0.01_0.9_0.0001_MAE:0.085806\n",
      "0.8_0.5_0.01_0.9_0.0001_MAPE:0.152801\n",
      "0.85_0.005_0.01_0.9_0.0001_RMSE:0.098687\n",
      "0.85_0.005_0.01_0.9_0.0001_MAE:0.079301\n",
      "0.85_0.005_0.01_0.9_0.0001_MAPE:0.137870\n",
      "0.85_0.005_0.01_0.9_0.0001\n",
      "best_evaluation: 0.3158567\n",
      "0.85_0.01_0.01_0.9_0.0001_RMSE:0.105068\n",
      "0.85_0.01_0.01_0.9_0.0001_MAE:0.085190\n",
      "0.85_0.01_0.01_0.9_0.0001_MAPE:0.149007\n",
      "0.85_0.05_0.01_0.9_0.0001_RMSE:0.107791\n",
      "0.85_0.05_0.01_0.9_0.0001_MAE:0.086507\n",
      "0.85_0.05_0.01_0.9_0.0001_MAPE:0.157123\n",
      "0.85_0.1_0.01_0.9_0.0001_RMSE:0.103300\n",
      "0.85_0.1_0.01_0.9_0.0001_MAE:0.083824\n",
      "0.85_0.1_0.01_0.9_0.0001_MAPE:0.143347\n",
      "0.85_0.5_0.01_0.9_0.0001_RMSE:0.102290\n",
      "0.85_0.5_0.01_0.9_0.0001_MAE:0.082792\n",
      "0.85_0.5_0.01_0.9_0.0001_MAPE:0.144789\n",
      "0.9_0.005_0.01_0.9_0.0001_RMSE:0.107802\n",
      "0.9_0.005_0.01_0.9_0.0001_MAE:0.086599\n",
      "0.9_0.005_0.01_0.9_0.0001_MAPE:0.156433\n",
      "0.9_0.01_0.01_0.9_0.0001_RMSE:0.109949\n",
      "0.9_0.01_0.01_0.9_0.0001_MAE:0.090009\n",
      "0.9_0.01_0.01_0.9_0.0001_MAPE:0.154541\n",
      "0.9_0.05_0.01_0.9_0.0001_RMSE:0.102944\n",
      "0.9_0.05_0.01_0.9_0.0001_MAE:0.083467\n",
      "0.9_0.05_0.01_0.9_0.0001_MAPE:0.145508\n",
      "0.9_0.1_0.01_0.9_0.0001_RMSE:0.107942\n",
      "0.9_0.1_0.01_0.9_0.0001_MAE:0.086812\n",
      "0.9_0.1_0.01_0.9_0.0001_MAPE:0.156574\n",
      "0.9_0.5_0.01_0.9_0.0001_RMSE:0.107050\n",
      "0.9_0.5_0.01_0.9_0.0001_MAE:0.086944\n",
      "0.9_0.5_0.01_0.9_0.0001_MAPE:0.152732\n",
      "0.95_0.005_0.01_0.9_0.0001_RMSE:0.107641\n",
      "0.95_0.005_0.01_0.9_0.0001_MAE:0.087407\n",
      "0.95_0.005_0.01_0.9_0.0001_MAPE:0.154392\n",
      "0.95_0.01_0.01_0.9_0.0001_RMSE:0.107421\n",
      "0.95_0.01_0.01_0.9_0.0001_MAE:0.086985\n",
      "0.95_0.01_0.01_0.9_0.0001_MAPE:0.155128\n",
      "0.95_0.05_0.01_0.9_0.0001_RMSE:0.105882\n",
      "0.95_0.05_0.01_0.9_0.0001_MAE:0.086036\n",
      "0.95_0.05_0.01_0.9_0.0001_MAPE:0.150873\n",
      "0.95_0.1_0.01_0.9_0.0001_RMSE:0.099841\n",
      "0.95_0.1_0.01_0.9_0.0001_MAE:0.080741\n",
      "0.95_0.1_0.01_0.9_0.0001_MAPE:0.138902\n",
      "0.95_0.5_0.01_0.9_0.0001_RMSE:0.106491\n",
      "0.95_0.5_0.01_0.9_0.0001_MAE:0.086434\n",
      "0.95_0.5_0.01_0.9_0.0001_MAPE:0.152786\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.8,0.85,0.9,0.95]   ####0.7,0.8,0.9,1.0\n",
    "ks = [0.005,0.01,0.05,0.1,0.5]   \n",
    "\n",
    "lr = 0.01               \n",
    "momentums =0.9                                       \n",
    "weight_decay =1e-4                                        \n",
    "\n",
    "num_feature = 7     #ETTh1:7,DJI:5\n",
    "batch_size = 256\n",
    "set_seed()\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1=256, hidden_size2=128,output_size=pred_length):   ###DJI:hidden_size=256,ETTh1:hidden_size=128.\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = FLinear(input_size, hidden_size1, alpha, k)  \n",
    "        self.leakrelu1 = nn.LeakyReLU()                          \n",
    "        self.linear2 = FLinear(hidden_size1, hidden_size2, alpha, k) \n",
    "        self.leakrelu2 = nn.LeakyReLU()\n",
    "        self.linear3 = FLinear(hidden_size2, output_size, alpha, k)   \n",
    "\n",
    "    def forward(self, x, epoch=0):\n",
    "        x = self.flatten(x)    # (batch_size, seq_len*num_features)\n",
    "        x = self.leakrelu1(self.linear1(x, epoch)) \n",
    "        x = self.leakrelu2(self.linear2(x, epoch))\n",
    "        x = self.linear3(x, epoch)\n",
    "        return x\n",
    "\n",
    "for alpha in alphas:\n",
    "    for k in ks:\n",
    "        set_seed()\n",
    "        model = MLP(input_size=slide_windows_size*num_feature).to(device)\n",
    "        num_epochs = 500   #\n",
    "        best_loss = 10\n",
    "        criterion = nn.MSELoss()\n",
    "        # optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-2)\n",
    "        optimizer = torch.optim.SGD(model.parameters(),lr=lr,momentum=momentum,weight_decay=weight_decay)\n",
    "        for ii in range(num_epochs):\n",
    "            model.train()\n",
    "            loss_sum = 0\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs,ii)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss_sum += loss\n",
    "                loss.backward()   #The default value of retain_graph is False.\n",
    "                optimizer.step()\n",
    "            # train_loss10.append(loss_sum.cpu().detach().numpy())     ###########\n",
    "            \n",
    "            # print(f\"Epoch {ii + 1}/{num_epochs}, Train Loss: {loss_sum.cpu().detach().numpy():.4f}\")\n",
    "                \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Val_outputs = model(X_val)\n",
    "                MSE_val = MSE(y_val.cpu().detach().numpy(),Val_outputs.cpu().detach().numpy())\n",
    "                \n",
    "                # val_loss10.append(MSE_val)   ########################Validation_loss\n",
    "                \n",
    "                # print(f\"Epoch {ii + 1}/{num_epochs}, Val Loss: {MSE_val:.4f}\")\n",
    "                # print('')\n",
    "                if best_loss > MSE_val:\n",
    "                    best_loss = MSE_val\n",
    "                    torch.save(model.state_dict(), r'./model/table2/'+stock+'_model_fractional_f_'+str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_.pth') \n",
    "\n",
    "        model.load_state_dict(torch.load('./model/table2/'+stock+'_model_fractional_f_'+str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_.pth'))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "        RMSE10 = RMSE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        MAE10 = MAE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        MAPE10 = MAPE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_'+f'RMSE:{RMSE10:.6f}')\n",
    "        print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_'+f'MAE:{MAE10:.6f}')\n",
    "        print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay)+'_'+f'MAPE:{MAPE10:.6f}')\n",
    "        if best_evaluation > RMSE10 + MAE10 + MAPE10:\n",
    "            best_evaluation = RMSE10 + MAE10 + MAPE10\n",
    "            print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(momentum)+'_'+str(weight_decay))\n",
    "            print('best_evaluation:',best_evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_12_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
