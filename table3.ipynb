{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "from torch import Tensor\n",
    "from scipy.special import gamma \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class Fractional_Order_Matrix_Differential_Solver(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,input1,w,b,alpha,k,epoch):\n",
    "        alpha = torch.tensor(alpha)\n",
    "        k = torch.tensor(k)\n",
    "        epoch = torch.tensor(epoch)\n",
    "        ctx.save_for_backward(input1,w,b,alpha,k,epoch)\n",
    "        outputs = input1@w + b\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        input1,w,b,alpha,k,epoch = ctx.saved_tensors\n",
    "        x_fractional, w_fractional = Fractional_Order_Matrix_Differential_Solver.Fractional_Order_Matrix_Differential_Linear(input1,w,b,alpha,k,epoch)   \n",
    "        x_grad = torch.mm(grad_outputs,x_fractional)\n",
    "        w_grad = torch.mm(w_fractional,grad_outputs)\n",
    "        b_grad = grad_outputs.sum(dim=0)\n",
    "        return x_grad, w_grad, b_grad,None,None,None\n",
    "\n",
    "    @staticmethod\n",
    "    def Fractional_Order_Matrix_Differential_Linear(x,w,b,alpha,k,epoch):\n",
    "        #w\n",
    "        wf = w[:,0].view(1,-1)\n",
    "        #main\n",
    "        w_main = torch.mul(x,(torch.abs(wf)+1e-8)**(1-alpha)/gamma(2-alpha))\n",
    "        #partial\n",
    "        x_rows, x_cols = x.size()\n",
    "        bias = torch.full((x_rows, x_cols),b[0].item())\n",
    "        bias = bias.to(device)\n",
    "        w_partial = torch.mul(torch.mm(x,wf.T).view(-1,1).expand(-1,x_cols) - torch.mul(x,wf) + bias, torch.sign(wf)*(torch.abs(wf)+1e-8)**(-alpha)/gamma(1-alpha))\n",
    "        return w.T, (w_main + torch.exp(-k*epoch)*w_partial).T\n",
    "\n",
    "class FLinear(nn.Module):\n",
    "    \n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, alpha=0.9, k = 0.9, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "        self.weight = Parameter(torch.empty((in_features, out_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x, epoch):\n",
    "        return Fractional_Order_Matrix_Differential_Solver.apply(x, self.weight, self.bias, self.alpha, self.k, epoch)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}\"\n",
    "    \n",
    "def split(X,y):\n",
    "    X_train,X_temp,y_train,y_temp = train_test_split(X,y,test_size=0.3,shuffle=False)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333,shuffle=False)\n",
    "    return X_train,X_val,X_test,y_train,y_val,y_test\n",
    "\n",
    "#Mean Square Error\n",
    "def MSE(pred,true):\n",
    "    return np.mean((pred-true)**2)\n",
    "\n",
    "#Mean Absolute Error\n",
    "def MAE(pred, true):\n",
    "    return np.mean(np.abs(pred-true))\n",
    "\n",
    "def RMSE(pred,true):\n",
    "    return np.sqrt(np.mean((pred-true)**2))\n",
    "\n",
    "def MAPE(pred, true):\n",
    "    return np.mean(np.abs((pred - true) / true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_windows_size = 192  #i.e.,input length 192\n",
    "pred_length = 384     #i.e.,prediction lengths 384\n",
    "stock = 'ETTh2'    #ETTh2,DJI\n",
    "df_DJIA = pd.read_csv(r'./data/'+stock+'.csv')\n",
    "del df_DJIA['date']        #ETT2\n",
    "# del df_DJIA['Date']        #DJI\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "sca_DJIA = scaler.fit_transform(df_DJIA)\n",
    "\n",
    "features_j = 6     #ETTh2:6,DJI:4\n",
    "def create_sequences(data, slide_windows_size, pred_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - slide_windows_size - pred_length + 1):\n",
    "        X.append(data[i:i+slide_windows_size, :])  # sliding window size [seq_len, features]\n",
    "        y.append(data[i+slide_windows_size:i+slide_windows_size+pred_length, features_j])  \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(sca_DJIA, slide_windows_size, pred_length)\n",
    "X = torch.Tensor(X).to(device)\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train,X_val,X_test,y_train,y_val,y_test = split(X,y)   #7:2:1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0   ####0.7,0.8,0.9,1.0\n",
    "k = 0.01      #In integer order, k does not play a role.\n",
    "\n",
    "lrs =[0.01,0.005,0.001,0.0005]                     \n",
    "weight_decays =[0.1,0.01,0.001,0.0001]                                           \n",
    "\n",
    "num_feature = 7     #ETTh1:7,DJI:5\n",
    "batch_size = 256\n",
    "set_seed()\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1=256, hidden_size2=128,output_size=pred_length):   ###DJI:hidden_size=256,ETTh1:hidden_size=128.\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # self.linear1 = FLinear(input_size, hidden_size1, alpha, k)  \n",
    "        self.linear1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.leakrelu1 = nn.LeakyReLU()                          \n",
    "        # self.linear2 = FLinear(hidden_size1, hidden_size2, alpha, k) \n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.leakrelu2 = nn.LeakyReLU()\n",
    "        # self.linear3 = FLinear(hidden_size2, output_size, alpha, k)   \n",
    "        self.linear3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x, epoch=0):\n",
    "        x = self.flatten(x)    # (batch_size, seq_len*num_features)\n",
    "        # x = self.leakrelu1(self.linear1(x, epoch)) \n",
    "        x = self.leakrelu1(self.linear1(x))\n",
    "        # x = self.leakrelu2(self.linear2(x, epoch))\n",
    "        x = self.leakrelu2(self.linear2(x))\n",
    "        # x = self.linear3(x, epoch)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "lr_best = 0\n",
    "weight_decay_best = 0\n",
    "best_evaluation = 10\n",
    "\n",
    "for lr in lrs:\n",
    "    for weight_decay in weight_decays:\n",
    "        set_seed()\n",
    "        model = MLP(input_size=slide_windows_size*num_feature).to(device)\n",
    "        num_epochs = 100   #\n",
    "        best_loss = 10\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "        for ii in range(num_epochs):\n",
    "            model.train()\n",
    "            loss_sum = 0\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs,ii)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss_sum += loss\n",
    "                loss.backward()   #The default value of retain_graph is False.\n",
    "                optimizer.step()\n",
    "            # train_loss10.append(loss_sum.cpu().detach().numpy())     ###########\n",
    "            \n",
    "            # print(f\"Epoch {ii + 1}/{num_epochs}, Train Loss: {loss_sum.cpu().detach().numpy():.4f}\")\n",
    "                \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Val_outputs = model(X_val)\n",
    "                MSE_val = MSE(y_val.cpu().detach().numpy(),Val_outputs.cpu().detach().numpy())\n",
    "                \n",
    "                # val_loss10.append(MSE_val)   ########################Validation_loss\n",
    "                \n",
    "                # print(f\"Epoch {ii + 1}/{num_epochs}, Val Loss: {MSE_val:.4f}\")\n",
    "                # print('')\n",
    "                if best_loss > MSE_val:\n",
    "                    best_loss = MSE_val\n",
    "                    torch.save(model.state_dict(), r'./model/tableA1/'+stock+'_model_fractional_'+str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(weight_decay)+'_.pth') \n",
    "\n",
    "        model.load_state_dict(torch.load('./model/tableA1/'+stock+'_model_fractional_'+str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(weight_decay)+'_.pth'))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "        RMSE10 = RMSE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        MAE10 = MAE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        MAPE10 = MAPE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(weight_decay)+'_'+f'RMSE:{RMSE10:.4f}')\n",
    "        print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(weight_decay)+'_'+f'MAE:{MAE10:.4f}')\n",
    "        print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(weight_decay)+'_'+f'MAPE:{MAPE10:.4f}')\n",
    "        if best_evaluation > RMSE10 + MAE10 + MAPE10:\n",
    "            best_evaluation = RMSE10 + MAE10 + MAPE10\n",
    "            print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(weight_decay))\n",
    "            print(f'best_evaluation:{best_evaluation:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.8,0.85,0.9,0.95]   ####0.7,0.8,0.9,1.0\n",
    "ks = [0.005,0.01,0.05,0.1,0.5]   \n",
    "\n",
    "lr = 0.001                                                \n",
    "weight_decay =0.0001                                 \n",
    "\n",
    "num_feature = 7     #ETTh1:7,DJI:5\n",
    "batch_size = 256\n",
    "set_seed()\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1=256, hidden_size2=128,output_size=pred_length):   ###DJI:hidden_size=256,ETTh1:hidden_size=128.\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = FLinear(input_size, hidden_size1, alpha, k)  \n",
    "        self.leakrelu1 = nn.LeakyReLU()                          \n",
    "        self.linear2 = FLinear(hidden_size1, hidden_size2, alpha, k) \n",
    "        self.leakrelu2 = nn.LeakyReLU()\n",
    "        self.linear3 = FLinear(hidden_size2, output_size, alpha, k)   \n",
    "\n",
    "    def forward(self, x, epoch=0):\n",
    "        x = self.flatten(x)    # (batch_size, seq_len*num_features)\n",
    "        x = self.leakrelu1(self.linear1(x, epoch)) \n",
    "        x = self.leakrelu2(self.linear2(x, epoch))\n",
    "        x = self.linear3(x, epoch)\n",
    "        return x\n",
    "\n",
    "for alpha in alphas:\n",
    "    for k in ks:\n",
    "        set_seed()\n",
    "        model = MLP(input_size=slide_windows_size*num_feature).to(device)\n",
    "        num_epochs = 100   #\n",
    "        best_loss = 10\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "        for ii in range(num_epochs):\n",
    "            model.train()\n",
    "            loss_sum = 0\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs,ii)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss_sum += loss\n",
    "                loss.backward()   #The default value of retain_graph is False.\n",
    "                optimizer.step()\n",
    "            # train_loss10.append(loss_sum.cpu().detach().numpy())     ###########\n",
    "            \n",
    "            # print(f\"Epoch {ii + 1}/{num_epochs}, Train Loss: {loss_sum.cpu().detach().numpy():.4f}\")\n",
    "                \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Val_outputs = model(X_val)\n",
    "                MSE_val = MSE(y_val.cpu().detach().numpy(),Val_outputs.cpu().detach().numpy())\n",
    "                \n",
    "                # val_loss10.append(MSE_val)   ########################Validation_loss\n",
    "                \n",
    "                # print(f\"Epoch {ii + 1}/{num_epochs}, Val Loss: {MSE_val:.4f}\")\n",
    "                # print('')\n",
    "                if best_loss > MSE_val:\n",
    "                    best_loss = MSE_val\n",
    "                    torch.save(model.state_dict(), r'./model/tableA1/'+stock+'_model_fractional_f_'+str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(weight_decay)+'_.pth') \n",
    "\n",
    "        model.load_state_dict(torch.load('./model/tableA1/'+stock+'_model_fractional_f_'+str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(weight_decay)+'_.pth'))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "        RMSE10 = RMSE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        MAE10 = MAE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        MAPE10 = MAPE(y_test.cpu().numpy(),test_outputs.cpu().detach().numpy())\n",
    "        print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(weight_decay)+'_'+f'RMSE:{RMSE10:.4f}')\n",
    "        print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(weight_decay)+'_'+f'MAE:{MAE10:.4f}')\n",
    "        print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(weight_decay)+'_'+f'MAPE:{MAPE10:.4f}')\n",
    "        if best_evaluation > RMSE10 + MAE10 + MAPE10:\n",
    "            best_evaluation = RMSE10 + MAE10 + MAPE10\n",
    "            print(str(alpha)+'_'+str(k)+'_'+str(lr)+'_'+str(weight_decay))\n",
    "            print(f'best_evaluation:{best_evaluation:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_12_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
