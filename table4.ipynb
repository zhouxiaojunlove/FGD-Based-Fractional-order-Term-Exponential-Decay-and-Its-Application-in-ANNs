{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Memory: 912.4282 MB\n",
      "Peak Memory: 945.2031 MB\n",
      "Memory: 32.7749 MB\n",
      "Training time: 2.2853 s\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "from torch import Tensor\n",
    "from scipy.special import gamma \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#Mean Square Error\n",
    "def MSE(pred,true):\n",
    "    return np.mean((pred-true)**2)\n",
    "\n",
    "#Mean Absolute Error\n",
    "def MAE(pred, true):\n",
    "    return np.mean(np.abs(pred-true))\n",
    "\n",
    "class Fractional_Order_Matrix_Differential_Solver(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,input1,w,b,alpha,k,epoch):\n",
    "        alpha = torch.tensor(alpha)\n",
    "        k = torch.tensor(k)\n",
    "        epoch = torch.tensor(epoch)\n",
    "        ctx.save_for_backward(input1,w,b,alpha,k,epoch)\n",
    "        outputs = input1@w + b\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        input1,w,b,alpha,k,epoch = ctx.saved_tensors\n",
    "        x_fractional, w_fractional = Fractional_Order_Matrix_Differential_Solver.Fractional_Order_Matrix_Differential_Linear(input1,w,b,alpha,k,epoch)   \n",
    "        x_grad = torch.mm(grad_outputs,x_fractional)\n",
    "        w_grad = torch.mm(w_fractional,grad_outputs)\n",
    "        b_grad = grad_outputs.sum(dim=0)\n",
    "        return x_grad, w_grad, b_grad,None,None,None\n",
    "\n",
    "    @staticmethod\n",
    "    def Fractional_Order_Matrix_Differential_Linear(x,w,b,alpha,k,epoch):\n",
    "        #w\n",
    "        wf = w[:,0].view(1,-1)\n",
    "        #main\n",
    "        w_main = torch.mul(x,(torch.abs(wf)+1e-8)**(1-alpha)/gamma(2-alpha))\n",
    "        #partial\n",
    "        x_rows, x_cols = x.size()\n",
    "        bias = torch.full((x_rows, x_cols),b[0].item())\n",
    "        bias = bias.to(device)\n",
    "        w_partial = torch.mul(torch.mm(x,wf.T).view(-1,1).expand(-1,x_cols) - torch.mul(x,wf) + bias, torch.sign(wf)*(torch.abs(wf)+1e-8)**(-alpha)/gamma(1-alpha))\n",
    "        return w.T, (w_main + torch.exp(-k*epoch)*w_partial).T\n",
    "\n",
    "class FLinear(nn.Module):\n",
    "    \n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, alpha=0.9, k = 0.9, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "        self.weight = Parameter(torch.empty((in_features, out_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x, epoch):\n",
    "        return Fractional_Order_Matrix_Differential_Solver.apply(x, self.weight, self.bias, self.alpha, self.k, epoch)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}\"\n",
    "    \n",
    "def split(X,y):\n",
    "    X_train,X_temp,y_train,y_temp = train_test_split(X,y,test_size=0.3,shuffle=False)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333,shuffle=False)\n",
    "    return X_train,X_val,X_test,y_train,y_val,y_test\n",
    "\n",
    "slide_windows_size = 192  #i.e.,input length \n",
    "pred_length = 384     #i.e.,prediction lengths \n",
    "stock = 'ETTm2'\n",
    "df_DJIA = pd.read_csv(r'./data/'+stock+'.csv')\n",
    "del df_DJIA['date']        #ETT\n",
    "# scaler = StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "sca_DJIA = scaler.fit_transform(df_DJIA)\n",
    "\n",
    "\n",
    "features_j = 6     \n",
    "def create_sequences(data, slide_windows_size, pred_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - slide_windows_size - pred_length + 1):\n",
    "        X.append(data[i:i+slide_windows_size, :])  # sliding window size [seq_len, features]\n",
    "        y.append(data[i+slide_windows_size:i+slide_windows_size+pred_length, features_j])  \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(sca_DJIA, slide_windows_size, pred_length)\n",
    "X = torch.Tensor(X).to(device)\n",
    "y = torch.Tensor(y).to(device)\n",
    "\n",
    "X_train,X_val,X_test,y_train,y_val,y_test = split(X,y)   #7:2:1 \n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "set_seed()\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size1=256, hidden_size2=128,output_size=pred_length):   ###DJI:hidden_size=256,ETTh1:hidden_size=128.\n",
    "#         super().__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear1 = nn.Linear(input_size, hidden_size1)       \n",
    "#         self.leakrelu1 = nn.LeakyReLU()                          \n",
    "#         self.linear2 = nn.Linear(hidden_size1, hidden_size2) \n",
    "#         self.leakrelu2 = nn.LeakyReLU()\n",
    "#         self.linear3 = nn.Linear(hidden_size2, output_size)       \n",
    "#         #self.linear2 = nn.Linear(hidden_size, output_size)                                 #To calculate the Memory usage on SGD.\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)    # (batch_size, seq_len*num_features)\n",
    "#         x = self.leakrelu1(self.linear1(x))  \n",
    "#         x = self.leakrelu2(self.linear2(x))\n",
    "#         x = self.linear3(x)\n",
    "#         return x\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1=256, hidden_size2=128,output_size=pred_length):   ###DJI:hidden_size=256,ETTh1:hidden_size=128.\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = FLinear(input_size, hidden_size1, alpha, k)       \n",
    "        self.leakrelu1 = nn.LeakyReLU()                          \n",
    "        self.linear2 = FLinear(hidden_size1, hidden_size2, alpha, k) \n",
    "        self.leakrelu2 = nn.LeakyReLU()\n",
    "        self.linear3 = FLinear(hidden_size2, output_size, alpha, k)       \n",
    "        #self.linear2 = nn.Linear(hidden_size, output_size)                                 #To calculate the Memory usage on SGD.\n",
    "\n",
    "    def forward(self, x, epoch=0):\n",
    "        x = self.flatten(x)    # (batch_size, seq_len*num_features)\n",
    "        x = self.leakrelu1(self.linear1(x, epoch))  \n",
    "        x = self.leakrelu2(self.linear2(x, epoch))\n",
    "        x = self.linear3(x, epoch)\n",
    "        return x\n",
    "\n",
    "alpha = 1.0   ####0.7,0.8,0.9,1.0\n",
    "k = 0.01\n",
    "num_feature = 7  \n",
    "\n",
    "\n",
    "\n",
    "peak_memory_max = 0\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "initial_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "\n",
    "time_start = time.time()\n",
    "set_seed()\n",
    "model = MLP(input_size=slide_windows_size*num_feature).to(device)\n",
    "\n",
    "lr =1e-3\n",
    "num_epochs = 10   #1500\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for ii in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx,(inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()   #The default value of retain_graph is False.\n",
    "        optimizer.step()\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "        if peak_memory_max < peak_memory:\n",
    "            peak_memory_max = peak_memory\n",
    "\n",
    "time_end = time.time()\n",
    "print(f\"Initial Memory: {initial_memory:.4f} MB\")\n",
    "print(f\"Peak Memory: {peak_memory_max:.4f} MB\")\n",
    "print(f\"Memory: {(peak_memory_max-initial_memory):.4f} MB\")\n",
    "print(f\"Training time: {(time_end-time_start)/10:.4f} s\")\n",
    "###########To ensure fair comparisons, restart the kernel before running each experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_12_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
